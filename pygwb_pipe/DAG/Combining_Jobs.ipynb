{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forbidden-beast",
   "metadata": {},
   "source": [
    "# Combining pygwb_pipe jobs\n",
    "### (Max Lalleman, Arianna Renzini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63f2b7-2117-43d8-a0d5-1cf220909f7d",
   "metadata": {},
   "source": [
    "## 1. Loading in packages and path to pygwb and the output files from all the jobs\n",
    "\n",
    "If pygwb is not installed locally, the path to the package has to be provided below.\n",
    "\n",
    "**NOTE: Running this notebook on the full 100 days takes ~30 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74971c91-c5ab-41a0-b3bd-4c22e5c96a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "'''\n",
    "module_path_2 = '/home/max.lalleman/public_html/Code/New_Clone_pyGWB/pygwb'\n",
    "sys.path.insert(0,module_path_2)\n",
    "'''\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7eb368-95f7-473e-b76e-91bcfed86f5f",
   "metadata": {},
   "source": [
    "We define here the path to the output files from all jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93459597-f625-4340-9d13-8e58b5f82e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/max.lalleman/public_html/Code/New_Clone_pyGWB/pygwb/DAG/output/Tryout_100_day/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f79c2-1584-4f7d-a3ec-d81d8ec31cb4",
   "metadata": {},
   "source": [
    "We now load in the files found in the path folder, both psds and point_estimate files.\n",
    "\n",
    "Currently this is not yet implemented for the pickled baseline files, but the procedure for reading in and using their data will be quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3511b5-fa49-47dd-910b-63643ee69d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles_psd = [f for f in listdir(path) if isfile(join(path, f)) if f.startswith(\"psds\")]\n",
    "onlyfiles_point_estimates = [f for f in listdir(path) if isfile(join(path, f)) if f.startswith(\"point\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745297c5-77fd-42c9-8a50-b2b23b59df58",
   "metadata": {},
   "source": [
    "The reading of these files is not automatically sorted, so we have to sort our lists of data files using a certain sortingFunction which depends on the naming convention of those data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8286a2-4bd5-4382-bf4a-f2123c2d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortingFunction(item):\n",
    "    return np.float64(item[10:].partition('.')[0])   \n",
    "\n",
    "def sortingFunction_point(item):\n",
    "    return np.float64(item[21:].partition('.')[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c7978-d0c7-4f0a-bd4f-fadecffca0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles_psd.sort(key=sortingFunction)\n",
    "data_files_psd = np.array(onlyfiles_psd)\n",
    "\n",
    "onlyfiles_point_estimates.sort(key=sortingFunction_point)\n",
    "data_files_point = np.array(onlyfiles_point_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225cd15-db0a-443f-b844-e3253e0f9ead",
   "metadata": {},
   "source": [
    "Now we have datafiles of psds and point_estimates in chronological order.\n",
    "\n",
    "We load in just the first file from this list to get the frequencies for later -- these are the same for all files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fae04c-f240-4aba-8003-482efe4550aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_freq = data_files_point[0]\n",
    "data_Freq = np.load(\"{0}\".format(path)+data_file_freq)\n",
    "frequencies = data_Freq['frequencies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b25dfe-ce76-428a-ba45-5e4a4c0afed0",
   "metadata": {},
   "source": [
    "## 2. Reading in the output from pygwb_pipe: Y_f and sigma_f\n",
    "\n",
    "Now we will load in the data from these data_files. I have tried to optimise this process, because it can take a very long time.\n",
    "\n",
    "So far this is the fastest method I have found, using a pre-defined loading function and list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb487d-f510-4974-822e-3098018602cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_function(path, file):\n",
    "    data_file = np.load(\"{0}\".format(path) + file)\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1bea7-1d00-4f22-bb88-5a3a7cd0d1a4",
   "metadata": {},
   "source": [
    "In what follows, we restrict the analysis to the first 30 days of output data. You can find the full analysis in the commented sections of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = 100\n",
    "days_to_load = 30\n",
    "last_idx = int(len(data_files_point)*days_to_load/total_days)\n",
    "\n",
    "print(f'We\\'re going to load the first {last_idx} out of the {len(data_files_point)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e921d-ce71-4b57-838c-9b90e21d29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_opt_from_each_job_short = [float(loading_function(path, file)[\"point_estimate\"]) for file in data_files_point[0:last_idx]]\n",
    "Y_opt_from_each_job_short = np.array(Y_opt_from_each_job_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6927d0b-eb5e-490f-a5f0-baf7f8343b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_opt_from_each_job_short = [float(loading_function(path, file)[\"sigma\"]) for file in data_files_point[0:last_idx]]\n",
    "sig_opt_from_each_job_short = np.array(sig_opt_from_each_job_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6854789-fd42-42ac-bed4-041a09cf95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_segment_from_each_job_short = [loading_function(path, file)[\"point_estimate_spectrogram\"] for file in data_files_point[0:last_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a132b-4222-4162-b2dc-dc630d89c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_segment_from_each_job_short = [loading_function(path, file)[\"sigma_spectrogram\"] for file in data_files_point[0:last_idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992aa6fc-f10b-444b-a693-9b512e3de63d",
   "metadata": {},
   "source": [
    "The loading in (which is the longest part of this notebook) of this 'short' list should take around four minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e0cde-da0b-4d9e-8637-0b0a267dd539",
   "metadata": {},
   "source": [
    "And now the code which loads the full 100 days - **This is now commented out as it takes a long time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0daac-c394-42bd-a0f2-b8da51fe41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Y_opt_from_each_job = [float(loading_function(path, file)[\"point_estimate\"]) for file in data_files_point]\n",
    "Y_opt_from_each_job = np.array(Y_opt_from_each_job)\n",
    "\n",
    "sig_opt_from_each_job = [float(loading_function(path, file)[\"sigma\"]) for file in data_files_point]\n",
    "sig_opt_from_each_job = np.array(sig_opt_from_each_job)\n",
    "\n",
    "Y_segment_from_each_job = [loading_function(path, file)[\"point_estimate_spectrogram\"] for file in data_files_point]\n",
    "\n",
    "sig_segment_from_each_job = [loading_function(path, file)[\"sigma_spectrogram\"] for file in data_files_point]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca685651-fb38-4d28-93be-5a42b8109ea9",
   "metadata": {},
   "source": [
    "We will continue our investigation by reading in the spectra of point estimate and sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf0c07-3604-4a54-954a-d17b9c60233f",
   "metadata": {},
   "source": [
    "##### first 30 days for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c25d92-44ce-47d5-897d-26f383870719",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_spectra_fast = np.zeros((len(data_files_point[0:last_idx]),len(frequencies)))\n",
    "for index,file in enumerate(data_files_point[0:last_idx]):\n",
    "    data_file = np.load(\"{0}\".format(path+file))\n",
    "    Y_spectra_fast[index,:] = data_file[\"point_estimate_spectrum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dea964-5f1b-452d-855f-f4c7e2ad1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_spectra_fast = np.zeros((len(data_files_point[0:last_idx]),len(frequencies)))\n",
    "for index,file in enumerate(data_files_point[0:last_idx]):\n",
    "    data_file = np.load(\"{0}\".format(path+file))\n",
    "    sigma_spectra_fast[index,:] = data_file[\"sigma_spectrum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c641b19-3e66-4b52-95d4-f38441439736",
   "metadata": {},
   "source": [
    "##### Full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bd7a3-b3d4-403e-8c35-2aedb8cb3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Y_spectra = np.zeros((len(data_files_point),len(frequencies)))\n",
    "for index,file in enumerate(data_files_point):\n",
    "    data_file = np.load(\"{0}\".format(path+file))\n",
    "    Y_spectra[index,:] = data_file[\"point_estimate_spectrum\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829f60f-4fb2-4229-848e-e5440d9ef49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sigma_spectra = np.zeros((len(data_files_point),len(frequencies)))\n",
    "for index,file in enumerate(data_files_point):\n",
    "    data_file = np.load(\"{0}\".format(path+file))\n",
    "    sigma_spectra[index,:] = data_file[\"sigma_spectrum\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a792a7c-6cce-43f3-9080-ffe7a9037b6a",
   "metadata": {},
   "source": [
    "## 3. Combining the data: $Y(f)$ , $\\sigma(f)$ , $Y$ , $\\sigma$ , and SNR\n",
    "\n",
    "We have to combine the data from each job to form the overall point estimate $Y$ and sigma $\\sigma$, and also to form the final point estimate and sigma spectra.\n",
    "\n",
    "We begin by loading in and combining the data from the spectrograms such that we can get the point estimate and sigma from each segment of the job files (*not per job, per segment*).\n",
    "\n",
    "We perform that by using the *calc_Y_sigma_from_Yf_varf* function from `pygwb.util` that combines the spectra of Y and sigma for each segment into a single weighted value. To perform this calculation we need to define an $\\alpha$ and $f_{\\rm ref}$, and use the frequencies read in earlier. \n",
    "\n",
    "The list *Y_and_sig_per_segment* will have multiple elements (corresponding to each data segment) with each element having two entries: $Y$ and $\\sigma$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55028b0-be6c-4f1d-affe-575fa321a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygwb.postprocessing import calc_Y_sigma_from_Yf_varf\n",
    "fref = 25\n",
    "alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5048bc1-9f53-4cbc-b86c-16d3a08ce33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_and_sig_per_segment =  np.array([calc_Y_sigma_from_Yf_varf(row,sig_segment_from_each_job_short[index][second_index]**2,freqs=frequencies,alpha=alpha,fref=fref) \n",
    "                          for index,arr in enumerate(Y_segment_from_each_job_short) for second_index,row in enumerate(arr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef74ad1-22ee-4f9e-93ec-3305aadc53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_segment = Y_and_sig_per_segment.T[0]\n",
    "sig_segment = Y_and_sig_per_segment.T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cfb74-71ef-4f80-bb89-4bef5cad1d4a",
   "metadata": {},
   "source": [
    "We define here the means and standard deviations of *Y_segment* and *sig_segment*, which are used later for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928b660-a863-4288-9f9c-aff1c9d77efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_segment = np.mean(Y_segment)\n",
    "std_Y_segment = np.std(Y_segment)\n",
    "gem_sig_segment = np.mean(sig_segment)\n",
    "std_sig_segment = np.std(sig_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e121e-edd3-4553-94de-ee0b41b702ca",
   "metadata": {},
   "source": [
    "We now combine the point estimates and sigmas of every job into one final point estimate and sigma. The jobs are treated as independent *segments* - therefore we can use the relative formulas for example shown in the O3 isotropic paper.\n",
    "\n",
    "Specifically we use\n",
    "\n",
    "$$\n",
    "Y_{\\rm final} =  \\frac{\\sum_{\\rm jobs}\\frac{Y_{\\rm job}}{\\sigma^2_{\\rm job}}}{\\sum_{{\\rm jobs}} \\frac{1}{\\sigma_{\\rm job}^2}}\\, , \\qquad \\sigma_{\\rm final} = \\sqrt{\\frac{1}{\\sum_{{\\rm jobs}} \\frac{1}{\\sigma_{\\rm job}^2}}}\n",
    "$$\n",
    "\n",
    "However, we also want to make plots of $Y$ and $\\sigma$ per day. <br>\n",
    "Since the jobs were ran for 7200 seconds and a day is 12 times 7200 = 86400 seconds we take a sum over 12 elements for plotting later on.\n",
    "\n",
    "The final values are called *Y_final* and *sigma_final*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cb012-16c8-49ec-ac65-a8c2226900b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_opt_squared = [sigma**-2 for sigma in sig_opt_from_each_job_short]\n",
    "inv_var_sum = np.sum(sig_opt_squared)\n",
    "\n",
    "#Overall sigma\n",
    "sigma_final = np.sqrt(1/inv_var_sum)\n",
    "\n",
    "inv_var_sum_day = [np.sum(sig_opt_squared[i:i + 12]) for i in range(0, len(sig_opt_squared), 12)]\n",
    "sig_opt_day = [np.sqrt(1/ele) for ele in inv_var_sum_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353a16f-9baa-4748-83fd-56401de32d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_opt_all = [Y_est * sigma**-2 for Y_est, sigma in zip(Y_opt_from_each_job_short, sig_opt_from_each_job_short)]\n",
    "Y_est_100_day_first_step = [np.sum(Y_opt_all[i:i + 12]) for i in range(0, len(Y_opt_all), 12)]\n",
    "\n",
    "Y_est_100_day_end = [ele/sigma for ele, sigma in zip(Y_est_100_day_first_step, inv_var_sum_day)]\n",
    "#Overall point Estimate\n",
    "Y_final = np.sum(Y_opt_all)/inv_var_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final values: {:.2e}\".format(Y_final)+ \" +\\- {:.2e}\".format(sigma_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1959871-35ff-48fe-b68a-3813fe5a0105",
   "metadata": {},
   "source": [
    "Now we combine the spectra into one final spectrum. The final spectra are called *Y_f_final* and *sigma_f_final*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593a1d-39c4-45e2-96f9-7b88160376a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_transposed = sigma_spectra_fast.T\n",
    "Y_transposed = Y_spectra_fast.T\n",
    "\n",
    "sig_opt_squared_f = [[1/sig**2 for sig in row] for row in var_transposed]\n",
    "\n",
    "var_sum_f = np.sum(sig_opt_squared_f,axis=1)\n",
    "sigma_f_final = np.sqrt(1/var_sum_f)\n",
    "\n",
    "Y_opt_total_f = [[Y_est * sigma**-2 for Y_est,sigma in zip(row,row2)] for row, row2 in zip(Y_transposed,var_transposed)]\n",
    "Y_f_final = np.sum(Y_opt_total_f,axis=1)/var_sum_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dd625-ce2c-42a8-bcc6-0449b56e4e57",
   "metadata": {},
   "source": [
    "Finally, we also compute the SNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25accdf6-3ab0-4ea6-a5e8-c517c0733a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR = np.zeros(len(Y_est_100_day_end))\n",
    "for index, ele in enumerate(Y_est_100_day_end):\n",
    "    #print(f\"Y = {ele:e} \\u00B1 {sig_opt_total_new[index]:e} \\n\")\n",
    "    SNR_new = ele/sig_opt_day[index]\n",
    "    SNR[index] = SNR_new\n",
    "    #print(f\"SNR is equal to {SNR_new:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226f072-5342-4953-ad79-bcbfdd16af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_seg = np.zeros(len(Y_segment))\n",
    "for index, ele in enumerate(Y_segment):\n",
    "    #print(f\"Y = {ele:e} \\u00B1 {sig_opt_total_new[index]:e} \\n\")\n",
    "    SNR_new = ele/sig_segment[index]\n",
    "    SNR_seg[index] = SNR_new\n",
    "    #print(f\"SNR is equal to {SNR_new:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b3ae-4cf6-43b3-bcfb-ac03d89d7afc",
   "metadata": {},
   "source": [
    "Once again for plotting purposes, we store some means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190904ff-9e09-4ef3-9921-5c9d1be9f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_SNR = np.mean(SNR)\n",
    "std_SNR = np.std(SNR)\n",
    "gem_SNR_seg = np.mean(SNR_seg)\n",
    "std_SNR_seg = np.std(SNR_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0429a67-ab14-4a82-81bb-bfd06749733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_day = np.mean(Y_est_100_day_end)\n",
    "std_day = np.std(Y_est_100_day_end)\n",
    "gem_daily_sig = np.mean(sig_opt_day)\n",
    "std_daily_sig = np.std(sig_opt_day)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0748cfc-c68e-4ee1-8df9-9e8dcaeac9fa",
   "metadata": {},
   "source": [
    "## 4. Plotting\n",
    "\n",
    "We will start by plotting some of the results we have achieved in this notebook. <br>\n",
    "First of all the SNR histograms. They might be a bit wacky, because we do not use a lot of data, but they might give a idea of how it should look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4c16d-712a-4821-82bd-50647bc2096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(SNR, bins = 10)\n",
    "plt.title('SNR histo day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_SNR, color= 'black', linestyle='--', label=\"$\\\\mu_{\\mathrm{SNR}}=$ \"+\"{:.1f}\".format(gem_SNR) + \"\\n$\\\\sigma_{\\mathrm{SNR}}=$ \"+\"{:.1f}\".format(std_SNR))\n",
    "plt.xticks(size = 30)\n",
    "plt.yticks(size = 30)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32476fcf-ecf6-42bb-833c-a87948baa3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(SNR_seg, bins = 50)\n",
    "plt.title('SNR histo per segment for day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_SNR_seg, color= 'black', linestyle='--', label = \"$\\\\mu_{\\mathrm{SNR}}=$ \"+\"{:.1f}\".format(gem_SNR_seg) + \"\\n$\\\\sigma_{\\mathrm{SNR}}=$ \"+\"{:.1f}\".format(std_SNR_seg))\n",
    "plt.xticks(size = 30)\n",
    "plt.yticks(size = 30)\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506fec2-d50c-41f3-8173-df16ec0be229",
   "metadata": {},
   "source": [
    "We can do the same for the point estimate and sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165c914-bc1e-4870-91df-e65c93235c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_est_100_day_end, bins = 10)\n",
    "plt.title('Y histo day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_day, color= 'black', linestyle='--', label = \"$\\\\mu_{\\mathrm{Y}}=$ \"+\"{:.2e}\".format(gem_day) + \"\\n$\\\\sigma_{\\mathrm{Y}}=$ \"+\"{:.2e}\".format(std_day))\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1b775-ef7e-4e2a-b26c-055a20e5764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_segment, bins = 50)\n",
    "plt.title('Y histo per segment for day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_segment, color= 'black', linestyle='--', label = \"$\\\\mu_{\\mathrm{Y}}=$ \"+\"{:.2e}\".format(gem_segment) + \"\\n$\\\\sigma_{\\mathrm{Y}}=$ \"+\"{:.2e}\".format(std_Y_segment))\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f64815-af07-48b6-b40b-d57b184f2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sig_opt_day, bins = 10)\n",
    "plt.title('$\\sigma$ histo day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_daily_sig, color= 'black', linestyle='--', label = \"$\\\\mu_{\\mathrm{\\\\sigma}}=$ \"+\"{:.2e}\".format(gem_daily_sig) + \"\\n$\\\\sigma_{\\mathrm{\\\\sigma}}=$ \"+\"{:.2e}\".format(std_daily_sig))\n",
    "plt.xticks(size = 25)\n",
    "plt.yticks(size = 30)\n",
    "plt.xlabel('$\\sigma$')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81e8dc-39d8-4e9f-aebd-bb8f35ff6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sig_segment, bins = 50)\n",
    "plt.xticks(size = 20)\n",
    "plt.title('$\\sigma$ histo per segment for day 1-30 pyGWB', fontsize = 20)\n",
    "plt.axvline(gem_sig_segment, color= 'black', linestyle='--', label = \"$\\\\mu_{\\mathrm{\\\\sigma}}=$ \"+\"{:.2e}\".format(gem_sig_segment) + \"\\n$\\\\sigma_{\\mathrm{\\\\sigma}}=$ \"+\"{:.2e}\".format(std_sig_segment))\n",
    "plt.yticks(size = 20)\n",
    "plt.xlabel('$\\sigma$')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92b1be-5048-4da1-a859-39795d3862ce",
   "metadata": {},
   "source": [
    "We can also make the cumulative plot for the point estimate and sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3a09a-e763-4999-b71d-dc80ae588fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_opt_day = np.array(sig_opt_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bafae0-e2c0-48aa-8afc-c46b897cad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_cum_num = np.cumsum(Y_est_100_day_end/(sig_opt_day**2))\n",
    "Y_cum_den = np.cumsum(1/(sig_opt_day**2))\n",
    "  \n",
    "Y_cum = Y_cum_num/Y_cum_den\n",
    "sigma_cum = 1./np.sqrt(np.cumsum(1/(sig_opt_day**2)))\n",
    "SNR_cum = Y_cum/sigma_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906dae8-c26a-4619-9017-96698a3754a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Y = \", Y_cum[-1], \"+/-\", sigma_cum[-1], \"\\nSNR = \", SNR_cum[-1])\n",
    "days = range(1,days_to_load+1)\n",
    "plt.plot(days, Y_cum, label = \"Y cumulative\", linewidth = 3)\n",
    "plt.plot(days, Y_cum + sigma_cum, label = \"Y + sigma cumulative\", linewidth = 3)\n",
    "plt.plot(days, Y_cum - sigma_cum, label = \"Y - sigma cumulative\", linewidth = 3)\n",
    "plt.xlim(0, days[-1])\n",
    "plt.title(r\"$Y \\pm \\sigma$ cumulative day {}-{} pyGWB\".format(days[0], days[-1]), fontsize = 20)\n",
    "plt.xticks(size = 30)\n",
    "plt.yticks(size = 30)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(days, SNR_cum, label = \"SNR cumulative\")\n",
    "plt.xlim(0, days[-1])\n",
    "plt.title(\"SNR cumulative day {}-{} pyGWB\".format(days[0], days[-1]), fontsize = 20)\n",
    "plt.xticks(size = 30)\n",
    "plt.yticks(size = 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10446d26-c05f-45a5-a997-7348078cf6a1",
   "metadata": {},
   "source": [
    "And also plot the final spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f18a81-04e8-4d94-aac4-351284ac5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_Y = 1.06e-7\n",
    "plt.plot(frequencies, Y_f_final)\n",
    "plt.plot(frequencies, sigma_f_final + injected_Y, 'k')\n",
    "plt.plot(frequencies, - sigma_f_final + injected_Y, 'k')\n",
    "#plt.plot(frequencies, sig_opt_total_new_f,'k')\n",
    "#plt.plot(frequencies, - sig_opt_total_new_f,'k')\n",
    "plt.xlim(20, 100)\n",
    "plt.ylim(-5e-6,5e-6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef05f6-29d7-4eb8-94a4-b37002e8fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}